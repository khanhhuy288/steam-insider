{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9545eb",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Use a list of **app_id** to get info from Steam crawler and insert **app_id**, **game_name**, **header_img_url**, **total_positive**, **total_negative**, **total_reviews** into table **games** and insert **game_id**, **review**, **recommended**, **time** into table **reviews**.\n",
    "- Preprocess **review** in table **reviews** (add_missing_punct, replace_bullets, remove_url, remove_html_tags, normalize_single_quote, remove_non_ascii, remove_ansi_escape_sequences, remove_multi_whitespaces), then tokenize_sent and remove_leading_symbols to insert **review_id**, **sent** into table **sents**. \n",
    "- Preprocess **sent** in table **sents** (lowercase, expand contractions, remove_digits, remove_symbols, remove_multi_whitespaces, lemmatize_text, remove_stopwords) to create **sent_prep** in table **sents**.\n",
    "- Use **sent_prep**, **review_id** in table **sents** to insert **review_prep** in table **reviews** by joining **sent_prep**.\n",
    "- Use **review_prep** in table **reviews** to calculate special bigrams frequency, get 50 most frequent keywords.\n",
    "- Insert **kw**, **freq** into table **kws**.\n",
    "- Embed 50 keywords using S-BERT and cluster them using agglomerative clustering with a distance_threshold=0.6. \n",
    "\t- Insert **cluster_name** (name of the most frequent keyword in cluster) into table **clusters**.\n",
    "\t- Insert **cluster_id** in table **kws**.\n",
    "- Loop through **sent_prep** in table **sents**, fuzzy-match each **kw** in table **kws**. \n",
    "    - Insert **cluster_id**, **sent_id** in table **clusters_sents**t to link table **clusters** and **sents**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c2602",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5a441",
   "metadata": {},
   "source": [
    "- Use a list of **app_id** to get info from Steam crawler and insert **app_id**, **game_name**, **header_img_url**, **total_positive**, **total_negative**, **total_reviews** into table **games** and insert **game_id** (fk), **review**, **recommended**, **time** into table **reviews**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38323474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\steam_insider\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-27 18:32:33,006 loading file C:\\Users\\HuyTran\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import download_steam_reviews\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from fuzzysearch import find_near_matches\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import flair\n",
    "\n",
    "sentiment_model = flair.models.TextClassifier.load('sentiment')\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80071b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contractions import contractions\n",
    "contractions = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11a1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./data/steam_reviews.db') \n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ebfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cursor(cursor): \n",
    "    print(cursor.execute(\"\"\"\n",
    "        select * from games;\n",
    "    \"\"\").fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cursor(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cursor.execute(\"\"\"\n",
    "        select game_id from games \n",
    "        where app_id=367520;\n",
    "    \"\"\").fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80fb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 428550 - Momodora: Reverie under the Moonlight\n",
    "# 367520 - Hollow Knight\n",
    "app_ids = [428550, 367520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f35a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get game_name ###\n",
    "\n",
    "def get_app_list(): \n",
    "    app_list_url = 'https://api.steampowered.com/ISteamApps/GetAppList/v2/'\n",
    "    resp_data = requests.get(app_list_url)\n",
    "    return resp_data.json()\n",
    "\n",
    "def get_name(app_id, app_list): \n",
    "    for app in app_list['applist']['apps']: \n",
    "        if app['appid'] == app_id: \n",
    "            return app['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78465edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get header_img_url  \n",
    "\n",
    "def get_header_img_url(app_id): \n",
    "    return f'https://cdn.cloudflare.steamstatic.com/steam/apps/{app_id}/header.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get total_positive, total_negative, total_reviews in Crawler for table games \n",
    "### get game_id, review, recommended, time in Crawler for table games \n",
    "\n",
    "app_list = get_app_list()\n",
    "\n",
    "request_params = {\n",
    "    'language': 'english'\n",
    "}\n",
    "\n",
    "# load or download new (maximum ~5000 newest reviews)\n",
    "load_mode = True\n",
    "\n",
    "for app_id in app_ids: \n",
    "    game_tuples = [] \n",
    "\n",
    "    game_name = get_name(app_id, app_list)\n",
    "    \n",
    "    header_img_url = get_header_img_url(app_id)\n",
    "    \n",
    "    total_positive, total_negative, total_reviews = 0, 0, 0\n",
    "    \n",
    "    if load_mode: \n",
    "        review_dict = download_steam_reviews.load_review_dict(app_id)['reviews'].values()\n",
    "    else: \n",
    "        review_dict = download_steam_reviews.download_reviews_for_app_id(app_id, \n",
    "                                                                     chosen_request_params=request_params, \n",
    "                                                                     reviews_limit=5000)[0]['reviews'].values()\n",
    "    \n",
    "    review_tuples = []\n",
    "    \n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"INSERT INTO games (app_id, game_name, header_img_url) VALUES (?, ?, ?);\"\"\", \n",
    "                       (app_id, game_name, header_img_url))\n",
    "    \n",
    "    # get game_id (fk) for table reviews\n",
    "    game_id = cursor.execute(\"\"\"SELECT game_id FROM games WHERE app_id=?;\"\"\", (app_id,)).fetchone()[0] \n",
    "    \n",
    "    for review_dict_value in review_dict: \n",
    "        total_reviews += 1\n",
    "    \n",
    "        voted_up = 1 if review_dict_value['voted_up'] else 0\n",
    "    \n",
    "        if voted_up: \n",
    "            total_positive += 1\n",
    "        else: \n",
    "            total_negative += 1\n",
    "     \n",
    "        review = review_dict_value['review']\n",
    "        recommended = voted_up\n",
    "        time = review_dict_value['timestamp_updated']\n",
    "        \n",
    "        review_tuples.append((review, recommended, time, game_id))\n",
    "    \n",
    "    with conn:\n",
    "        cursor.execute(\"\"\"UPDATE games SET (total_positive, total_negative, total_reviews) = (?, ?, ?)\n",
    "                            WHERE game_id=?;\"\"\",\n",
    "                       (total_positive, total_negative, total_reviews, game_id))\n",
    "    \n",
    "    with conn:\n",
    "        cursor.executemany(\"\"\"INSERT INTO reviews (review, recommended, time, game_id) VALUES \n",
    "                                (?, ?, ?, ?);\"\"\", review_tuples)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f39413",
   "metadata": {},
   "source": [
    "### Step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05092c7",
   "metadata": {},
   "source": [
    "- Preprocess **review** in table **reviews** (add_missing_punct, replace_bullets, remove_url, remove_html_tags, normalize_single_quote, remove_non_ascii, remove_ansi_escape_sequences, remove_multi_whitespaces), then tokenize_sent and remove_leading_symbols to insert **review_id**, **sent** into table **sents**. \n",
    "- Preprocess **sent** in table **sents** (lowercase, expand contractions, remove_digits, remove_symbols, remove_multi_whitespaces, lemmatize_text, remove_stopwords) to insert **sent_prep** in table **sents**.\n",
    "- Use **sent_prep**, **review_id** in table **sents** to insert **review_prep** in table **reviews** by joining **sent_prep**.\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada075ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_sql_query(\"\"\"SELECT review_id, review, game_id \n",
    "                    FROM reviews JOIN games USING(game_id);\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ff242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21424b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_punct(text): \n",
    "    return re.sub('([A-Za-z0-9])\\s*$', '\\g<1>. ', text)\n",
    "\n",
    "\n",
    "def replace_bullets(text): \n",
    "    text = re.sub('([A-Za-z0-9])\\s*\\n+\\s*[+-]?\\s*', '\\g<1>. ', text)\n",
    "    text = re.sub('\\s*([:+-]+)\\s*\\n+\\s*[+-]?\\s*', '. ', text) \n",
    "    return text\n",
    "    \n",
    "    \n",
    "# remove url from text\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+\", ' ', text)\n",
    "\n",
    "\n",
    "# remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    text = soup.get_text()\n",
    "    # remove square brackets and characters inside\n",
    "    text = re.sub('\\[(.*?)\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# replace ’ with ' \n",
    "def normalize_single_quote(text):\n",
    "    return re.sub('[’‘]', '\\'', text)\n",
    "\n",
    "\n",
    "# remove non english characters effectively\n",
    "def remove_non_ascii(text): \n",
    "    return text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    \n",
    "    \n",
    "# remove ANSI escape sequences\n",
    "def remove_ansi_escape_sequences(text):\n",
    "    ansi_escape = re.compile(r'(?:\\x1B[@-_]|[\\x80-\\x9F])[0-?]*[ -/]*[@-~]')\n",
    "    return ansi_escape.sub('', text)\n",
    "    \n",
    "    \n",
    "# remove multiple whitespaces with single whitespace\n",
    "def remove_multi_whitespaces(text): \n",
    "    return re.sub('\\s+', ' ', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_symbols(sent):\n",
    "    return re.sub('^[^A-Za-z\\\"\\'\\d]+', '', sent)\n",
    "\n",
    "def uppercase_first(sent): \n",
    "    return sent[0].upper() + sent[1:] if len(sent) != 0 else sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(text):\n",
    "    doc = nlp(text, disable=['ner', 'attribute_ruler', 'lemmatizer', 'sentencizer'])\n",
    "    sents = [str(sent).strip() for sent in doc.sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c734f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for key in contractions:\n",
    "        value = contractions[key]\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "# remove digits \n",
    "def remove_digits(text): \n",
    "    return re.sub('\\d+', ' ', text)\n",
    "\n",
    "# remove symbols \n",
    "def remove_symbols(text):\n",
    "    return re.sub('[^A-Za-z,.\\s\\d]+', ' ', text)\n",
    "\n",
    "# lemmatization with spacy \n",
    "def lemmatize_text(text): \n",
    "    doc = nlp(text, disable=['parser','ner'])\n",
    "    lemma = [token.lemma_ for token in doc if token.pos_ != 'PUNCT']\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "# remove stop words \n",
    "def remove_stopwords(text, word_list=[]):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words.extend(word_list)\n",
    "    stop_words = set(stop_words)\n",
    "    return ' '.join(e.lower() for e in text.split() if e.lower() not in stop_words)\n",
    "\n",
    "def get_extra_stopwords(game_name): \n",
    "    stopwords = set(['game'])\n",
    "    doc = nlp(game_name.lower(), disable=['parser', 'ner'])\n",
    "    for token in doc: \n",
    "        if token.pos_ not in {'PUNCT', 'NUM'}:\n",
    "            stopwords.add(token.text)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2891e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tuples = []\n",
    "\n",
    "for game_id in df_reviews['game_id'].unique():\n",
    "    with conn: \n",
    "        game_name = cursor.execute(\"\"\"SELECT game_name FROM games WHERE game_id=?;\"\"\", (int(game_id),)).fetchone()[0]\n",
    "    \n",
    "    extra_stopwords = get_extra_stopwords(game_name)\n",
    "    \n",
    "    df_reviews_game = df_reviews[df_reviews['game_id'] == game_id]\n",
    "    \n",
    "    reviews_game_cleaned = df_reviews_game['review'].map(add_missing_punct)\\\n",
    "                    .map(replace_bullets)\\\n",
    "                    .map(remove_url)\\\n",
    "                    .map(remove_html_tags)\\\n",
    "                    .map(normalize_single_quote)\\\n",
    "                    .map(remove_non_ascii)\\\n",
    "                    .map(remove_ansi_escape_sequences)\\\n",
    "                    .map(remove_multi_whitespaces)\n",
    "    \n",
    "    for review_id, review in zip(df_reviews_game['review_id'], reviews_game_cleaned):\n",
    "        sents = pd.Series(tokenize_sent(review)).map(remove_leading_symbols)\\\n",
    "                                                .map(uppercase_first)\\\n",
    "                                                .map(add_missing_punct)\\\n",
    "                                                .map(remove_multi_whitespaces)\n",
    "\n",
    "        sents_prep = sents.map(lowercase)\\\n",
    "                        .map(expand_contractions)\\\n",
    "                        .map(remove_digits)\\\n",
    "                        .map(remove_symbols)\\\n",
    "                        .map(remove_multi_whitespaces)\\\n",
    "                        .map(lemmatize_text)\\\n",
    "                        .map(lambda x: remove_stopwords(x, word_list=extra_stopwords))\n",
    "\n",
    "        for sent, sent_prep in zip(sents, sents_prep):\n",
    "            sent_tuples.append((review_id, sent, sent_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b836f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"INSERT INTO sents (review_id, sent, sent_prep) VALUES \n",
    "                        (?, ?, ?);\"\"\", sent_tuples)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44af4a1",
   "metadata": {},
   "source": [
    "### Step 3: \n",
    "- Use **review_prep** in table **reviews** to calculate special bigrams frequency, get 50 most frequent keywords.\n",
    "- Insert **kw**, **freq** into table **kws**.\n",
    "- Embed 50 keywords using S-BERT and cluster them using agglomerative clustering with a distance_threshold=0.6. \n",
    "\t- Insert **cluster_name** (name of the most frequent keyword in cluster) into table **clusters**.\n",
    "\t- Insert **cluster_id** in table **kws**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba86193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(x, ngram, min_df=1):\n",
    "    vec = CountVectorizer(ngram_range=[ngram, ngram], min_df=min_df).fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    return words_freq\n",
    "\n",
    "def bigramRules(bigram): \n",
    "    first_pos = set(['ADJ', 'NOUN'])\n",
    "    second_pos = set(['NOUN'])\n",
    "    \n",
    "    tags = [token.pos_ for token in nlp(bigram, disable=['parser','ner'])]\n",
    "    \n",
    "    return tags[0] in first_pos and tags[1] in second_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_prep = pd.read_sql_query(\"\"\"SELECT sent_prep, game_id\n",
    "                    FROM sents JOIN reviews USING(review_id) JOIN games USING(game_id);\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e92b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tuples = []\n",
    "kw_tuples = []\n",
    "\n",
    "for game_id in df_sent_prep['game_id'].unique():\n",
    "    sents_prep = df_sent_prep[df_sent_prep['game_id'] == game_id]['sent_prep']\n",
    "    bigram_freq = get_n_gram(sents_prep, 2, 3)\n",
    "    bigram_df = pd.DataFrame(bigram_freq, columns=['bigram', 'freq'])\n",
    "    \n",
    "    bigram_df_50 = []\n",
    "    count = 0\n",
    "    \n",
    "    for bigram, freq in zip(bigram_df['bigram'], bigram_df['freq']): \n",
    "        if bigramRules(bigram):\n",
    "            bigram_df_50.append((bigram, freq))\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count == 50: \n",
    "                break\n",
    "                \n",
    "    bigram_df = pd.DataFrame(bigram_df_50, columns=['bigram', 'freq'])\n",
    "    \n",
    "    kws = bigram_df['bigram']\n",
    "    kw_embeddings = embedder.encode(kws)\n",
    "    \n",
    "    # Normalize the embeddings to unit length\n",
    "    kw_embeddings = kw_embeddings /  np.linalg.norm(kw_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # perform agglomerative clustering\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=None, affinity='cosine', linkage='average', distance_threshold=0.6)\n",
    "    clustering_model.fit(kw_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    \n",
    "    kw_clusters = {}\n",
    "    for kw_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in kw_clusters:\n",
    "            kw_clusters[cluster_id] = []\n",
    "\n",
    "        kw_clusters[cluster_id].append(kws[kw_id])\n",
    "        \n",
    "    bigram_df['cluster_num'] = cluster_assignment\n",
    "    \n",
    "    for cluster_num, cluster_val in kw_clusters.items():\n",
    "        cluster_tuples.append((int(game_id), int(cluster_num), cluster_val[0]))\n",
    "    \n",
    "    for kw, freq, cluster_num in zip(bigram_df['bigram'], bigram_df['freq'], bigram_df['cluster_num']):\n",
    "        kw_tuples.append((kw, freq, int(cluster_num), int(game_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"INSERT INTO clusters (game_id, cluster_num, cluster_name) VALUES \n",
    "                        (?, ?, ?);\"\"\", cluster_tuples)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64877",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"INSERT INTO kws (kw, freq, cluster_id) VALUES \n",
    "                        (?, ?, (SELECT cluster_id FROM clusters WHERE cluster_num=? AND game_id=?));\"\"\",\n",
    "                       kw_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab55ca",
   "metadata": {},
   "source": [
    "### Step 4: \n",
    "- Loop through **sent_prep** in table **sents**, fuzzy-match each **kw** in table **kws**. \n",
    "    - Insert **cluster_id**, **sent_id** in table **clusters_sents** to link table **clusters** and **sents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_prep = pd.read_sql_query(\"\"\"SELECT game_id, sent_prep, sent_id\n",
    "                    FROM sents JOIN reviews USING(review_id) JOIN games USING(game_id);\"\"\", conn)\n",
    "\n",
    "df_kw = pd.read_sql_query(\"\"\"SELECT game_id, cluster_id, kw \n",
    "                    FROM kws LEFT JOIN clusters USING(cluster_id);\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b549777",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sent_tuples = []\n",
    "\n",
    "for game_id in df_sent_prep['game_id'].unique():\n",
    "    sents_prep = df_sent_prep[df_sent_prep['game_id'] == game_id]\n",
    "    kws = df_kw[df_kw['game_id'] == game_id]\n",
    "    \n",
    "    kw_clusters = {}\n",
    "    for kw, cluster_id in zip(kws['kw'], kws['cluster_id']):\n",
    "        if cluster_id not in kw_clusters:\n",
    "            kw_clusters[cluster_id] = []\n",
    "\n",
    "        kw_clusters[cluster_id].append(kw)\n",
    "        \n",
    "    for sent_id, sent_prep in zip(sents_prep['sent_id'], sents_prep['sent_prep']):\n",
    "        for cluster_id, kws in kw_clusters.items():\n",
    "            for kw in kws:\n",
    "                matches = find_near_matches(kw, sent_prep, max_l_dist=1)\n",
    "\n",
    "                if len(matches) != 0: \n",
    "                    cluster_sent_tuples.append((cluster_id, sent_id))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"INSERT INTO clusters_sents  (cluster_id, sent_id) VALUES \n",
    "                        (?, ?);\"\"\", cluster_sent_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914e90c",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "- Remove all **sent_id** in table **sents** if they don't exist in table **clusters_sents**.\n",
    "    - Insert **score_flair**, **score_vader**, **recommended**, **score_total**, **sent_embedding** in table **sents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a39f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all sentences that contains no keyword from table sents\n",
    "with conn: \n",
    "    cursor.execute(\"\"\"\n",
    "        DELETE FROM sents \n",
    "        WHERE sent_id NOT IN (\n",
    "            SELECT DISTINCT sent_id\n",
    "            FROM clusters_sents);\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.read_sql_query(\"\"\"SELECT sent_id, sent, recommended\n",
    "                                FROM sents JOIN reviews USING(review_id);\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e052019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68045dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_flair(sent): \n",
    "    sent_flair = flair.data.Sentence(sent)\n",
    "    sentiment_model.predict(sent_flair)\n",
    "    \n",
    "    value_flair = sent_flair.labels[0].value\n",
    "    \n",
    "    return 1 if value_flair == 'POSITIVE' else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a582749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_vader(sent, threshold=0): \n",
    "    score_vader = senti_analyzer.polarity_scores(sent)['compound'] \n",
    "    return 1 if score_vader > threshold else -1 if score_vader < threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_voted(recommended): \n",
    "    return 1 if recommended else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21669cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_total(score_flair, score_vader, score_voted): \n",
    "    score_total = sum([score_flair, score_vader, score_voted])\n",
    "    \n",
    "    if score_vader == 0: \n",
    "        sentiment = 0\n",
    "    elif score_total < 0: \n",
    "        sentiment = -1\n",
    "    elif score_total > 1: \n",
    "        sentiment = 1\n",
    "    else: \n",
    "        sentiment = 0\n",
    "        \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b778dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_flair = df_sent['sent'].map(get_score_flair) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_vader = df_sent['sent'].map(get_score_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_voted = df_sent['recommended'].map(get_score_voted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9fd158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embeddings = embedder.encode(df_sent['sent'])\n",
    "\n",
    "sent_embeddings = sent_embeddings /  np.linalg.norm(sent_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "sent_embeddings_str = [pickle.dumps(sent_embedding) for sent_embedding in sent_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c13687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tuples = []\n",
    "\n",
    "for score_flair, score_vader, score_voted, sent_embedding, sent_id in zip(scores_flair, scores_vader, scores_voted, sent_embeddings, df_sent['sent_id']): \n",
    "    score_total = get_score_total(score_flair, score_vader, score_voted)\n",
    "    \n",
    "    sent_tuples.append((score_flair, score_vader, score_voted, score_total, sent_embedding, sent_id))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"UPDATE sents SET (score_flair, score_vader, score_voted, score_total, sent_embedding) = (?, ?, ?, ?, ?)\n",
    "                        WHERE sent_id=?;\"\"\", sent_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042b6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tuples = []\n",
    "\n",
    "for sent_embedding_str, sent_id in zip(sent_embeddings_str, df_sent['sent_id']): \n",
    "    sent_tuples.append((sent_embedding_str, sent_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b9b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    cursor.executemany(\"\"\"UPDATE sents SET sent_embedding = ?\n",
    "                        WHERE sent_id=?;\"\"\", sent_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf39527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_embedding = pd.read_sql_query(\"\"\"SELECT sent_embedding FROM sents;\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b36c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_embeddings = np.array([pickle.loads(sent_embedding) for sent_embedding in df_sent_embedding['sent_embedding']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3432c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04324087,  0.02180931,  0.01068075, ...,  0.03434775,\n",
       "        -0.04854265, -0.00114909],\n",
       "       [-0.03150932, -0.00418032,  0.04931263, ...,  0.06524638,\n",
       "        -0.02565461,  0.02001331],\n",
       "       [-0.05950123,  0.02035298,  0.03827898, ..., -0.00917198,\n",
       "        -0.0430759 ,  0.03698024],\n",
       "       ...,\n",
       "       [ 0.01159374,  0.00428933, -0.00926146, ...,  0.00592567,\n",
       "        -0.05686782,  0.05610381],\n",
       "       [ 0.05544416,  0.03463249, -0.03918877, ..., -0.0102831 ,\n",
       "        -0.02068716, -0.05278462],\n",
       "       [ 0.03634208, -0.00284135,  0.07634094, ...,  0.03640819,\n",
       "         0.01460779, -0.12906875]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9992b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Steam Insider",
   "language": "python",
   "name": "steam_insider"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
